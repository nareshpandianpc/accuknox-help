{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Welcome to AccuKnox Help What is AccuKnox \u00b6 AccuKnox is Identity-driven Security for Data, Kubernetes, and Native VM workloads for Private and Public Clouds. AccuKnox is Zero Trust Run-time Kubernetes Security Solution. It provides run-time protection for your Kubernetes and other cloud workloads using Kernel Native Primitives such as AppArmor SELinux and eBPF. Quick Start \u00b6 Read more \ud83d\udc49 Documentation - Getting Started Links \u00b6 Cilium KubeArmor","title":"Home"},{"location":"#what_is_accuknox","text":"AccuKnox is Identity-driven Security for Data, Kubernetes, and Native VM workloads for Private and Public Clouds. AccuKnox is Zero Trust Run-time Kubernetes Security Solution. It provides run-time protection for your Kubernetes and other cloud workloads using Kernel Native Primitives such as AppArmor SELinux and eBPF.","title":"What is AccuKnox"},{"location":"#quick_start","text":"Read more \ud83d\udc49 Documentation - Getting Started","title":"Quick Start"},{"location":"#links","text":"Cilium KubeArmor","title":"Links"},{"location":"license/","text":"License \u00b6 MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY","title":"License"},{"location":"license/#license","text":"MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY","title":"License"},{"location":"accuknox-onprem/agents-deploy/","text":"Add accuknox repository to install Agents helm package: \u00b6 helm repo add accuknox-agents https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-agents helm repo update helm search repo accuknox-agents Follow the below order to install agents on k8s cluster. Cilium \u00b6 Installation using Helm Step 1: helm repo add cilium https://helm.cilium.io/ Step 2: helm install cilium cilium/cilium --version 1 .10.5 --namespace kube-system FYR: https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-helm/ kArmor \u00b6 kArmor is a CLI client to help manage KubeArmor. KubeArmor is a container-aware runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers at the system level. Installation \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sh To build and install, clone the repository and make install \u00b6 FYR: https://github.com/kubearmor/kubearmor-client Shared-informer-agent \u00b6 Step 1 : Create namespace has accuknox-dev-shared-informer-agent Eg: kubectl create ns accuknox-dev-shared-informer-agent Step 2 : Install using helm helm upgrade --install shared-informer-agent-chart-1.0.2.tgz -n accuknox-dev-shared-informer-agent S3-audit-reporter \u00b6 Step 1 : Create namespace has accuknox-dev-s3-audit-reporter Eg. kubectl create ns accuknox-dev-s3-audit-reporter Step 2 : Install using helm Eg. helm upgrade --install s3-audit-reporter-charts-1.0.1.tgz -n accuknox-dev-s3-audit-reporter Feeder-Service \u00b6 Step 1 : Create namespace has feeder-service Eg. kubectl create ns feeder-service Step 2 : Install using helm Eg. helm upgrade --install feeder-service-0.1.0.tgz -n feeder-service Knox-Containersec \u00b6 Step 1 : Create namespace has accuknox-agents kubectl create ns accuknox-agents Step 2 : Install using helm Eg. helm upgrade --install knox-containersec-chart-0.1.0.tgz -n accuknox-agents Policy Enforcement Agent \u00b6 Install using helm helm upgrade --install policy-enforcement-agent-1.0.2.tgz -n accuknox-agents","title":"how to deploy?"},{"location":"accuknox-onprem/agents-deploy/#add_accuknox_repository_to_install_agents_helm_package","text":"helm repo add accuknox-agents https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-agents helm repo update helm search repo accuknox-agents Follow the below order to install agents on k8s cluster.","title":"Add accuknox repository to install Agents helm package:"},{"location":"accuknox-onprem/agents-deploy/#cilium","text":"Installation using Helm Step 1: helm repo add cilium https://helm.cilium.io/ Step 2: helm install cilium cilium/cilium --version 1 .10.5 --namespace kube-system FYR: https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-helm/","title":"Cilium"},{"location":"accuknox-onprem/agents-deploy/#karmor","text":"kArmor is a CLI client to help manage KubeArmor. KubeArmor is a container-aware runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers at the system level.","title":"kArmor"},{"location":"accuknox-onprem/agents-deploy/#installation","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sh","title":"Installation"},{"location":"accuknox-onprem/agents-deploy/#to_build_and_install_clone_the_repository_and_make_install","text":"FYR: https://github.com/kubearmor/kubearmor-client","title":"To build and install, clone the repository and make install"},{"location":"accuknox-onprem/agents-deploy/#shared-informer-agent","text":"Step 1 : Create namespace has accuknox-dev-shared-informer-agent Eg: kubectl create ns accuknox-dev-shared-informer-agent Step 2 : Install using helm helm upgrade --install shared-informer-agent-chart-1.0.2.tgz -n accuknox-dev-shared-informer-agent","title":"Shared-informer-agent"},{"location":"accuknox-onprem/agents-deploy/#s3-audit-reporter","text":"Step 1 : Create namespace has accuknox-dev-s3-audit-reporter Eg. kubectl create ns accuknox-dev-s3-audit-reporter Step 2 : Install using helm Eg. helm upgrade --install s3-audit-reporter-charts-1.0.1.tgz -n accuknox-dev-s3-audit-reporter","title":"S3-audit-reporter"},{"location":"accuknox-onprem/agents-deploy/#feeder-service","text":"Step 1 : Create namespace has feeder-service Eg. kubectl create ns feeder-service Step 2 : Install using helm Eg. helm upgrade --install feeder-service-0.1.0.tgz -n feeder-service","title":"Feeder-Service"},{"location":"accuknox-onprem/agents-deploy/#knox-containersec","text":"Step 1 : Create namespace has accuknox-agents kubectl create ns accuknox-agents Step 2 : Install using helm Eg. helm upgrade --install knox-containersec-chart-0.1.0.tgz -n accuknox-agents","title":"Knox-Containersec"},{"location":"accuknox-onprem/agents-deploy/#policy_enforcement_agent","text":"Install using helm helm upgrade --install policy-enforcement-agent-1.0.2.tgz -n accuknox-agents","title":"Policy Enforcement Agent"},{"location":"accuknox-onprem/core-components-deploy/","text":"Add accuknox repository to install Core Components helm package: \u00b6 helm repo add accuknox-onprem https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-onprem helm repo update helm search repo accuknox-onprem Follow the below order to install core components on k8s cluster User-management-service: \u00b6 Step 1 : Create namespace has accuknox-dev-user-mgmt Eg. kubectl create ns accuknox-dev-user-mgmt Step 2 : Install using helm Eg. helm upgrade --install user-management-service-0.1.0.tgz -n accuknox-dev-user-mgmt Agents-Auth-Service \u00b6 Step 1 : Create namespace has accuknox-dev-agents-auth-service Eg. kubectl create ns accuknox-dev-agents-auth-service Step 2 : Install using helm Eg. helm upgrade --install agents-auth-service-charts-0.1.0.tgz -n accuknox-dev-agents-auth-service Anomaly-detection-management \u00b6 Step 1 : Create namespace has accuknox-dev-ad-mgmt Eg. kubectl create ns accuknox-dev-ad-mgmt Step 2 : Install using helm Eg. helm upgrade --install anomaly-detection-mgmt-chart-0.1.0.tgz -n accuknox-dev-ad-mgmt Anomaly-detection-publisher-core \u00b6 Step 1 : Create namespace has accuknox-dev-ad-core Eg. kubectl create ns accuknox-dev-ad-core Step 2 : Install using helm Eg. helm upgrade --install anomaly-detection-publisher-core-chart-1.0.4.tgz -n accuknox-dev-ad-core Data-protection-mgmt \u00b6 Step 1 : Create namespace has accuknox-dev-dp-mgmt Eg. kubectl create ns accuknox-dev-dp-mgmt Step 2 : Install using helm Eg. helm upgrade --install data-protection-mgmt-0.1.0.tgz -n accuknox-dev-dp-mgmt Data-protection-core \u00b6 Step 1 : Create namespace has accuknox-dev-dp-core Eg. kubectl create ns accuknox-dev-dp-core Step 2 : Install using helm Eg. helm upgrade --install data-protection-core-1.0.4.tgz -n accuknox-dev-dp-core Data-protection-consumer \u00b6 Step 1 : Create namespace has accuknox-dev-dp-core (Since ns already this step is optional) Eg. kubectl create ns accuknox-dev-dp-core Step 2 : Install using helm Eg. helm upgrade --install data-protection-consumer-0.1.0.tgz -n accuknox-dev-dp-core S3-audit-report-consumer \u00b6 Step 1 : Create namespace has accuknox-dev-s3-audit-reporter-consumer Eg. kubectl create ns accuknox-dev-s3-audit-reporter-consumer Step 2 : Install using helm Eg. helm upgrade --install s3-audit-reporter-consumer-charts-0.1.0.tgz -n accuknox-dev-s3-audit-reporter-consumer Dp-db-audit-log-processor \u00b6 Step 1 : Create namespace has accuknox-dev-dp-core kubectl create ns accuknox-dev-dp-core Step 2 : Install using helm Eg. helm upgrade --install dp-db-audit-log-processor-chart-0.1.0.tgz -n accuknox-dev-dp-core Data-classification-pipeline-consumer \u00b6 Step 1 : Create namespace has accuknox-dev-data-classification-pipeline-consumer Eg. kubectl create ns accuknox-dev-data-classification-pipeline-consumer Step 2 : Install using helm Eg. helm upgrade --install data-classification-pipeline-consumer-chart-0.1.0.tgz -n accuknox-dev-data-classification-pipeline-consumer Cluster-management-service \u00b6 Step 1 : Create namespace has accuknox-dev-cluster-mgmt Eg. kubectl create ns accuknox-dev-cluster-mgmt Step 2 : Install using helm Eg. helm upgrade --install cluster-management-service-chart-0.1.0.tgz -n accuknox-dev-cluster-mgmt Agent-data-collector \u00b6 Step 1 : Create namespace has accuknox-dev-adc Eg. kubectl create ns accuknox-dev-adc Step 2 : Install using helm Eg. helm upgrade --install agent-data-collector-charts-0.1.0.tgz -n accuknox-dev-adc Cluster-onboarding-service \u00b6 Step 1 : Create namespace has accuknox-dev-cluster-onboard Eg. kubectl create ns accuknox-dev-cluster-onboard Step 2 : Install using helm Eg. helm upgrade --install cluster-onboarding-service-0.1.0.tgz -n accuknox-dev-cluster-onboard Cluster-entity-daemon \u00b6 Step 1 : Create namespace has accuknox-dev-cluster-entity-daemon Eg: kubectl create ns accuknox-dev-cluster-entity-daemon Step 2 : Install using helm Eg. helm upgrade --install cluster-entity-daemon-chart-0.1.0.tgz -n accuknox-dev-cluster-entity-daemon Shared-informer-service \u00b6 Step 1 : Create namespace has accuknox-dev-shared-informer-service Eg. kubectl create ns accuknox-dev-shared-informer-service Step 2 : Install using helm Eg. helm upgrade --install shared-informer-service-chart-0.1.0.tgz -n accuknox-dev-shared-informer-service Data-pipeline-api \u00b6 Step 1 : Create namespace has accuknox-dev-datapipeline-api Eg. kubectl create ns accuknox-dev-datapipeline-api Step 2 : Install using helm Eg. helm upgrade --install data-pipeline-api-charts-0.1.0.tgz -n accuknox-dev-datapipeline-api Datapipeline-temporal \u00b6 Step 1 : Create namespace has accuknox-dev-temporal Eg. kubectl create ns accuknox-dev-temporal Step 2 : Install using helm Eg. helm upgrade --install datapipeline-temporal-charts-0.1.0.tgz -n accuknox-dev-temporal Data-pipeline-samza-jobs \u00b6 Step 1 : Create namespace has accuknox-dev-samzajobs Eg. kubectl create ns accuknox-dev-samzajobs Step 2 : Install using helm Eg. helm upgrade --install datapipeline-samza-0.1.0.tgz -n accuknox-dev-samzajobs Feeder-grpc-server \u00b6 Step 1 : Create namespace has accuknox-dev-feeder-grpc-server Eg. kubectl create ns accuknox-dev-feeder-grpc-server Step 2 : Install using helm Eg. helm upgrade --install feeder-grpc-server-chart-0.1.0.tgz -n accuknox-dev-feeder-grpc-server Policy-service Step 1 : Create namespace has accuknox-dev-policy-service Eg. kubectl create ns accuknox-dev-policy-service Step 2 : Install using helm Eg. helm upgrade --install policy-service-charts-0.1.0.tgz -n accuknox-dev-policy-service Policy-daemon \u00b6 Step 1 : Create namespace has accuknox-dev-policy-daemon Eg. kubectl create ns accuknox-dev-policy-daemon Step 2 : Install using helm Eg. helm upgrade --install policy-daemon-charts-0.1.0.tgz -n accuknox-dev-policy-daemon Policy-provider-service \u00b6 Step 1 : Create namespace has accuknox-dev-policy-provider-service Eg. kubectl create ns accuknox-dev-policy-provider-service Step 2 : Install using helm Eg. helm upgrade --install policy-provider-service-0.1.0.tgz -n accuknox-dev-policy-provider-service Workload-identity-daemon \u00b6 Step 1 : Create namespace has accuknox-dev-workload-identity-daemon Eg. kubectl create ns accuknox-dev-workload-identity-daemon Step 2 : Install using helm Eg. helm upgrade --install workload-identity-daemon-chart-0.1.0.tgz -n accuknox-dev-workload-identity-daemon Recommended-policy-daemon \u00b6 Step 1 : Create namespace has accuknox-dev-recommended-policy-daemon Eg. kubectl create ns accuknox-dev-recommended-policy-daemon Step 2 : Install using helm Eg. helm upgrade --install recommended-policy-daemon-1.0.4.tgz -n accuknox-dev-recommended-policy-daemon Discoveredpolicy-daemon Step 1 : Create namespace has accuknox-dev-discovered-policy-daemon Eg. kubectl create ns accuknox-dev-discovered-policy-daemon Step 2 : Install using helm Eg. helm upgrade --install discoveredpolicy-daemon-charts-0.1.0.tgz -n accuknox-dev-discovered-policy-daemon Label-service \u00b6 Step 1 : Create namespace has accuknox-dev-label-service Eg. kubectl create ns accuknox-dev-label-service Step 2 : Install using helm Eg. helm upgrade --install label-service-chart-0.1.0.tgz -n accuknox-dev-label-service Label-daemon \u00b6 Step 1 : Create namespace has accuknox-dev-label-daemon Eg. kubectl create ns accuknox-dev-label-daemon Step 2 : Install using helm Eg. helm upgrade --install label-daemon-charts-1.0.4.tgz -n accuknox-dev-label-daemon Knox-auto-policy \u00b6 Step 1 : Create namespace has accuknox-dev-knoxautopolicy Eg. kubectl create ns accuknox-dev-knoxautopolicy Step 2 : Install using helm Eg. helm upgrade --install knox-auto-policy-chart.tgz -n accuknox-dev-knoxautopolicy","title":"how to deploy?"},{"location":"accuknox-onprem/core-components-deploy/#add_accuknox_repository_to_install_core_components_helm_package","text":"helm repo add accuknox-onprem https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-onprem helm repo update helm search repo accuknox-onprem Follow the below order to install core components on k8s cluster","title":"Add accuknox repository to install Core Components helm package:"},{"location":"accuknox-onprem/core-components-deploy/#user-management-service","text":"Step 1 : Create namespace has accuknox-dev-user-mgmt Eg. kubectl create ns accuknox-dev-user-mgmt Step 2 : Install using helm Eg. helm upgrade --install user-management-service-0.1.0.tgz -n accuknox-dev-user-mgmt","title":"User-management-service:"},{"location":"accuknox-onprem/core-components-deploy/#agents-auth-service","text":"Step 1 : Create namespace has accuknox-dev-agents-auth-service Eg. kubectl create ns accuknox-dev-agents-auth-service Step 2 : Install using helm Eg. helm upgrade --install agents-auth-service-charts-0.1.0.tgz -n accuknox-dev-agents-auth-service","title":"Agents-Auth-Service"},{"location":"accuknox-onprem/core-components-deploy/#anomaly-detection-management","text":"Step 1 : Create namespace has accuknox-dev-ad-mgmt Eg. kubectl create ns accuknox-dev-ad-mgmt Step 2 : Install using helm Eg. helm upgrade --install anomaly-detection-mgmt-chart-0.1.0.tgz -n accuknox-dev-ad-mgmt","title":"Anomaly-detection-management"},{"location":"accuknox-onprem/core-components-deploy/#anomaly-detection-publisher-core","text":"Step 1 : Create namespace has accuknox-dev-ad-core Eg. kubectl create ns accuknox-dev-ad-core Step 2 : Install using helm Eg. helm upgrade --install anomaly-detection-publisher-core-chart-1.0.4.tgz -n accuknox-dev-ad-core","title":"Anomaly-detection-publisher-core"},{"location":"accuknox-onprem/core-components-deploy/#data-protection-mgmt","text":"Step 1 : Create namespace has accuknox-dev-dp-mgmt Eg. kubectl create ns accuknox-dev-dp-mgmt Step 2 : Install using helm Eg. helm upgrade --install data-protection-mgmt-0.1.0.tgz -n accuknox-dev-dp-mgmt","title":"Data-protection-mgmt"},{"location":"accuknox-onprem/core-components-deploy/#data-protection-core","text":"Step 1 : Create namespace has accuknox-dev-dp-core Eg. kubectl create ns accuknox-dev-dp-core Step 2 : Install using helm Eg. helm upgrade --install data-protection-core-1.0.4.tgz -n accuknox-dev-dp-core","title":"Data-protection-core"},{"location":"accuknox-onprem/core-components-deploy/#data-protection-consumer","text":"Step 1 : Create namespace has accuknox-dev-dp-core (Since ns already this step is optional) Eg. kubectl create ns accuknox-dev-dp-core Step 2 : Install using helm Eg. helm upgrade --install data-protection-consumer-0.1.0.tgz -n accuknox-dev-dp-core","title":"Data-protection-consumer"},{"location":"accuknox-onprem/core-components-deploy/#s3-audit-report-consumer","text":"Step 1 : Create namespace has accuknox-dev-s3-audit-reporter-consumer Eg. kubectl create ns accuknox-dev-s3-audit-reporter-consumer Step 2 : Install using helm Eg. helm upgrade --install s3-audit-reporter-consumer-charts-0.1.0.tgz -n accuknox-dev-s3-audit-reporter-consumer","title":"S3-audit-report-consumer"},{"location":"accuknox-onprem/core-components-deploy/#dp-db-audit-log-processor","text":"Step 1 : Create namespace has accuknox-dev-dp-core kubectl create ns accuknox-dev-dp-core Step 2 : Install using helm Eg. helm upgrade --install dp-db-audit-log-processor-chart-0.1.0.tgz -n accuknox-dev-dp-core","title":"Dp-db-audit-log-processor"},{"location":"accuknox-onprem/core-components-deploy/#data-classification-pipeline-consumer","text":"Step 1 : Create namespace has accuknox-dev-data-classification-pipeline-consumer Eg. kubectl create ns accuknox-dev-data-classification-pipeline-consumer Step 2 : Install using helm Eg. helm upgrade --install data-classification-pipeline-consumer-chart-0.1.0.tgz -n accuknox-dev-data-classification-pipeline-consumer","title":"Data-classification-pipeline-consumer"},{"location":"accuknox-onprem/core-components-deploy/#cluster-management-service","text":"Step 1 : Create namespace has accuknox-dev-cluster-mgmt Eg. kubectl create ns accuknox-dev-cluster-mgmt Step 2 : Install using helm Eg. helm upgrade --install cluster-management-service-chart-0.1.0.tgz -n accuknox-dev-cluster-mgmt","title":"Cluster-management-service"},{"location":"accuknox-onprem/core-components-deploy/#agent-data-collector","text":"Step 1 : Create namespace has accuknox-dev-adc Eg. kubectl create ns accuknox-dev-adc Step 2 : Install using helm Eg. helm upgrade --install agent-data-collector-charts-0.1.0.tgz -n accuknox-dev-adc","title":"Agent-data-collector"},{"location":"accuknox-onprem/core-components-deploy/#cluster-onboarding-service","text":"Step 1 : Create namespace has accuknox-dev-cluster-onboard Eg. kubectl create ns accuknox-dev-cluster-onboard Step 2 : Install using helm Eg. helm upgrade --install cluster-onboarding-service-0.1.0.tgz -n accuknox-dev-cluster-onboard","title":"Cluster-onboarding-service"},{"location":"accuknox-onprem/core-components-deploy/#cluster-entity-daemon","text":"Step 1 : Create namespace has accuknox-dev-cluster-entity-daemon Eg: kubectl create ns accuknox-dev-cluster-entity-daemon Step 2 : Install using helm Eg. helm upgrade --install cluster-entity-daemon-chart-0.1.0.tgz -n accuknox-dev-cluster-entity-daemon","title":"Cluster-entity-daemon"},{"location":"accuknox-onprem/core-components-deploy/#shared-informer-service","text":"Step 1 : Create namespace has accuknox-dev-shared-informer-service Eg. kubectl create ns accuknox-dev-shared-informer-service Step 2 : Install using helm Eg. helm upgrade --install shared-informer-service-chart-0.1.0.tgz -n accuknox-dev-shared-informer-service","title":"Shared-informer-service"},{"location":"accuknox-onprem/core-components-deploy/#data-pipeline-api","text":"Step 1 : Create namespace has accuknox-dev-datapipeline-api Eg. kubectl create ns accuknox-dev-datapipeline-api Step 2 : Install using helm Eg. helm upgrade --install data-pipeline-api-charts-0.1.0.tgz -n accuknox-dev-datapipeline-api","title":"Data-pipeline-api"},{"location":"accuknox-onprem/core-components-deploy/#datapipeline-temporal","text":"Step 1 : Create namespace has accuknox-dev-temporal Eg. kubectl create ns accuknox-dev-temporal Step 2 : Install using helm Eg. helm upgrade --install datapipeline-temporal-charts-0.1.0.tgz -n accuknox-dev-temporal","title":"Datapipeline-temporal"},{"location":"accuknox-onprem/core-components-deploy/#data-pipeline-samza-jobs","text":"Step 1 : Create namespace has accuknox-dev-samzajobs Eg. kubectl create ns accuknox-dev-samzajobs Step 2 : Install using helm Eg. helm upgrade --install datapipeline-samza-0.1.0.tgz -n accuknox-dev-samzajobs","title":"Data-pipeline-samza-jobs"},{"location":"accuknox-onprem/core-components-deploy/#feeder-grpc-server","text":"Step 1 : Create namespace has accuknox-dev-feeder-grpc-server Eg. kubectl create ns accuknox-dev-feeder-grpc-server Step 2 : Install using helm Eg. helm upgrade --install feeder-grpc-server-chart-0.1.0.tgz -n accuknox-dev-feeder-grpc-server Policy-service Step 1 : Create namespace has accuknox-dev-policy-service Eg. kubectl create ns accuknox-dev-policy-service Step 2 : Install using helm Eg. helm upgrade --install policy-service-charts-0.1.0.tgz -n accuknox-dev-policy-service","title":"Feeder-grpc-server"},{"location":"accuknox-onprem/core-components-deploy/#policy-daemon","text":"Step 1 : Create namespace has accuknox-dev-policy-daemon Eg. kubectl create ns accuknox-dev-policy-daemon Step 2 : Install using helm Eg. helm upgrade --install policy-daemon-charts-0.1.0.tgz -n accuknox-dev-policy-daemon","title":"Policy-daemon"},{"location":"accuknox-onprem/core-components-deploy/#policy-provider-service","text":"Step 1 : Create namespace has accuknox-dev-policy-provider-service Eg. kubectl create ns accuknox-dev-policy-provider-service Step 2 : Install using helm Eg. helm upgrade --install policy-provider-service-0.1.0.tgz -n accuknox-dev-policy-provider-service","title":"Policy-provider-service"},{"location":"accuknox-onprem/core-components-deploy/#workload-identity-daemon","text":"Step 1 : Create namespace has accuknox-dev-workload-identity-daemon Eg. kubectl create ns accuknox-dev-workload-identity-daemon Step 2 : Install using helm Eg. helm upgrade --install workload-identity-daemon-chart-0.1.0.tgz -n accuknox-dev-workload-identity-daemon","title":"Workload-identity-daemon"},{"location":"accuknox-onprem/core-components-deploy/#recommended-policy-daemon","text":"Step 1 : Create namespace has accuknox-dev-recommended-policy-daemon Eg. kubectl create ns accuknox-dev-recommended-policy-daemon Step 2 : Install using helm Eg. helm upgrade --install recommended-policy-daemon-1.0.4.tgz -n accuknox-dev-recommended-policy-daemon Discoveredpolicy-daemon Step 1 : Create namespace has accuknox-dev-discovered-policy-daemon Eg. kubectl create ns accuknox-dev-discovered-policy-daemon Step 2 : Install using helm Eg. helm upgrade --install discoveredpolicy-daemon-charts-0.1.0.tgz -n accuknox-dev-discovered-policy-daemon","title":"Recommended-policy-daemon"},{"location":"accuknox-onprem/core-components-deploy/#label-service","text":"Step 1 : Create namespace has accuknox-dev-label-service Eg. kubectl create ns accuknox-dev-label-service Step 2 : Install using helm Eg. helm upgrade --install label-service-chart-0.1.0.tgz -n accuknox-dev-label-service","title":"Label-service"},{"location":"accuknox-onprem/core-components-deploy/#label-daemon","text":"Step 1 : Create namespace has accuknox-dev-label-daemon Eg. kubectl create ns accuknox-dev-label-daemon Step 2 : Install using helm Eg. helm upgrade --install label-daemon-charts-1.0.4.tgz -n accuknox-dev-label-daemon","title":"Label-daemon"},{"location":"accuknox-onprem/core-components-deploy/#knox-auto-policy","text":"Step 1 : Create namespace has accuknox-dev-knoxautopolicy Eg. kubectl create ns accuknox-dev-knoxautopolicy Step 2 : Install using helm Eg. helm upgrade --install knox-auto-policy-chart.tgz -n accuknox-dev-knoxautopolicy","title":"Knox-auto-policy"},{"location":"accuknox-onprem/istio-deploy/","text":"Istio Installion steps: \u00b6 Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS): curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .10.0 TARGET_ARCH = x86_64 sh - Move to the Istio package directory. For example, if the package is istio-1.11.3: cd istio-1.10.0 Create a namespace istio-system for Istio components: kubectl create namespace istio-system Install the Istio base chart which contains cluster-wide resources used by the Istio control plane: helm install istio-base manifests/charts/base -n istio-system Install the Istio discovery chart which deploys the istiod control plane service: helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system Install the Istio ingress gateway chart which contains the ingress gateway components: helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system Verifying the installation: \u00b6 Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running: kubectl get pods -n istio-system Installing Gateway \u00b6 Along with creating a service mesh, Istio allows you to manage gateways, which are Envoy proxies running at the edge of the mesh, providing fine-grained control over traffic entering and leaving the mesh. Add accuknox repositories to install istio helm package: \u00b6 helm repo add accuknox-prerequisites https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-prerequisites helm repo update helm search repo accuknox-prerequisites helm pull accuknox-prerequisites/istio-gateway-charts --untar Move to directory cd istio-gateway-charts Cert Manager \u00b6 Install cert-manager. Cert-manager will manage the certificates of gateway domains. Setup permissions \u00b6 When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources. kubectl create clusterrolebinding cluster-admin-binding --clusterrole = cluster-admin --user = $( gcloud config get-value core/account ) Install Cert-manager \u00b6 kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml Platform-istio-gateway \u00b6 Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the micro services under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager Create Gateway \u00b6 Find the Gateway IP INGRESS_HOST = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) This will give you a LoadBalancer IP echo ${ INGRESS_HOST } Create DNS \u00b6 Create A record for example (sample.example.com and keycloak.example.com) using LoadBalancer IP # Create a certificate Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honoring certificate signing requests. kubectl apply -f issuer.yaml kubectl get ClusterIssuer -n cert-manager # Should have Status as Ready A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determine what will be honoring the certificate request. kubectl apply -f cert.yaml kubectl get Certificate -n istio-system # Should have Status as Ready # Create gateway with SSL kubectl apply -f gateway-with-ssl.yaml ` [ No need to specify namespace ] # Apply Virtual Service A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. kubectl apply -f backend-api/virtual-service.yaml [No need to specify namespace] kubectl apply -f keycloak/virtual-service.yaml # [No need to specify namespace]","title":"How to deploy istio?"},{"location":"accuknox-onprem/istio-deploy/#istio_installion_steps","text":"Go to the Istio release page to download the installation file for your OS, or download and extract the latest release automatically (Linux or macOS): curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .10.0 TARGET_ARCH = x86_64 sh - Move to the Istio package directory. For example, if the package is istio-1.11.3: cd istio-1.10.0 Create a namespace istio-system for Istio components: kubectl create namespace istio-system Install the Istio base chart which contains cluster-wide resources used by the Istio control plane: helm install istio-base manifests/charts/base -n istio-system Install the Istio discovery chart which deploys the istiod control plane service: helm install istiod manifests/charts/istio-control/istio-discovery -n istio-system Install the Istio ingress gateway chart which contains the ingress gateway components: helm install istio-ingress manifests/charts/gateways/istio-ingress -n istio-system","title":"Istio Installion steps:"},{"location":"accuknox-onprem/istio-deploy/#verifying_the_installation","text":"Ensure all Kubernetes pods in istio-system namespace are deployed and have a STATUS of Running: kubectl get pods -n istio-system","title":"Verifying the installation:"},{"location":"accuknox-onprem/istio-deploy/#installing_gateway","text":"Along with creating a service mesh, Istio allows you to manage gateways, which are Envoy proxies running at the edge of the mesh, providing fine-grained control over traffic entering and leaving the mesh.","title":"Installing Gateway"},{"location":"accuknox-onprem/istio-deploy/#add_accuknox_repositories_to_install_istio_helm_package","text":"helm repo add accuknox-prerequisites https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-prerequisites helm repo update helm search repo accuknox-prerequisites helm pull accuknox-prerequisites/istio-gateway-charts --untar Move to directory cd istio-gateway-charts","title":"Add accuknox repositories to install istio helm package:"},{"location":"accuknox-onprem/istio-deploy/#cert_manager","text":"Install cert-manager. Cert-manager will manage the certificates of gateway domains.","title":"Cert Manager"},{"location":"accuknox-onprem/istio-deploy/#setup_permissions","text":"When running on GKE (Google Kubernetes Engine), you might encounter a \u2018permission denied\u2019 error when creating some of the required resources. kubectl create clusterrolebinding cluster-admin-binding --clusterrole = cluster-admin --user = $( gcloud config get-value core/account )","title":"Setup permissions"},{"location":"accuknox-onprem/istio-deploy/#install_cert-manager","text":"kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.yaml","title":"Install Cert-manager"},{"location":"accuknox-onprem/istio-deploy/#platform-istio-gateway","text":"Istio Gateway configurations for DNS This gateway config file defines the base API endpoints of the micro services under DNS This repository also contains necessary files to setup SSL for DNS (Refer issuer.yaml and cert.yaml) using cert-manager","title":"Platform-istio-gateway"},{"location":"accuknox-onprem/istio-deploy/#create_gateway","text":"Find the Gateway IP INGRESS_HOST = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) This will give you a LoadBalancer IP echo ${ INGRESS_HOST }","title":"Create Gateway"},{"location":"accuknox-onprem/istio-deploy/#create_dns","text":"Create A record for example (sample.example.com and keycloak.example.com) using LoadBalancer IP # Create a certificate Issuers, and ClusterIssuers, are Kubernetes resources that represent certificate authorities (CAs) that are able to generate signed certificates by honoring certificate signing requests. kubectl apply -f issuer.yaml kubectl get ClusterIssuer -n cert-manager # Should have Status as Ready A Certificate is a namespaced resource that references an Issuer or ClusterIssuer that determine what will be honoring the certificate request. kubectl apply -f cert.yaml kubectl get Certificate -n istio-system # Should have Status as Ready # Create gateway with SSL kubectl apply -f gateway-with-ssl.yaml ` [ No need to specify namespace ] # Apply Virtual Service A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset/version of it) defined in the registry. kubectl apply -f backend-api/virtual-service.yaml [No need to specify namespace] kubectl apply -f keycloak/virtual-service.yaml # [No need to specify namespace]","title":"Create DNS"},{"location":"accuknox-onprem/istio/","text":"Istio is an open source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. Istio is the path to load balancing, service-to-service authentication, and monitoring \u2013 with few or no service code changes. Its powerful control plane brings vital features, including: Secure service-to-service communication in a cluster with TLS encryption, strong identity based authentication and authorization Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection A pluggable policy layer and configuration API supporting access controls, rate limits and quotas Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress For more reference: click here..","title":"What is Istio?"},{"location":"accuknox-onprem/kafka-deploy/","text":"Note: \u00b6 Kafka Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - kafka:true Labels - kafka:true Add accuknox repository to install strimzi-kafka-operator helm package: helm repo add accuknox-prerequisites https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-prerequisites helm repo update helm search repo accuknox-prerequisites helm pull accuknox-prerequisites/strimzi-kafka-operator --untar kubectl create namespace accuknox-dev-kafka kubectl config set-context --current --namespace = accuknox-dev-kafka helm install dev-kafka strimzi-kafka-operator Check the Pods deployment kubectl get pods -n accuknox-dev-kafka Extract the connectivity information. Get bootstrap server endpoint \u00b6 kubectl get kafka dev-kafka -o jsonpath = '{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-dev-kafka Get CA \u00b6 kubectl get secret dev-kafka-cluster-ca-cert -o jsonpath = '{.data.ca\\.p12}' -n accuknox-dev-kafka | base64 -d > ca.p12 Get CA Password \u00b6 kubectl get secret dev-kafka-cluster-ca-cert -o jsonpath = '{.data.ca\\.password}' -n accuknox-dev-kafka | base64 -d > ca.password Get User Cert \u00b6 kubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath = '{.data.user\\.p12}' | base64 -d > user.p12 Get user password \u00b6 kubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath = '{.data.user\\.password}' | base64 -d > user.password Convert user.p12 into base64 \u00b6 cat user.p12 | base64 > user.p12.base64 Convert ca.p12 into base64 \u00b6 cat ca.p12 | base64 > ca.p12.base64 Convert ca.password into base64 \u00b6 cat ca.password | base64 > ca.password.base64 Convert user.password into base64 \u00b6 cat user.password | base64 > user.password.base64 Convert p12 to pem \u00b6 openssl pkcs12 -in ca.p12 -out ca.pem Convert ca.pem to base64 \u00b6 cat ca.pem | base64 > ca.pem.base64 Note: ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files. FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity. FQDN : dev-kafka-kafka-bootstrap.accuknox-dev-kafka.svc.cluster.local:9092","title":"How to deploy Kafka?"},{"location":"accuknox-onprem/kafka-deploy/#note","text":"Kafka Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - kafka:true Labels - kafka:true Add accuknox repository to install strimzi-kafka-operator helm package: helm repo add accuknox-prerequisites https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-prerequisites helm repo update helm search repo accuknox-prerequisites helm pull accuknox-prerequisites/strimzi-kafka-operator --untar kubectl create namespace accuknox-dev-kafka kubectl config set-context --current --namespace = accuknox-dev-kafka helm install dev-kafka strimzi-kafka-operator Check the Pods deployment kubectl get pods -n accuknox-dev-kafka Extract the connectivity information.","title":"Note:"},{"location":"accuknox-onprem/kafka-deploy/#get_bootstrap_server_endpoint","text":"kubectl get kafka dev-kafka -o jsonpath = '{.status.listeners[?(@.type==\"external\")].bootstrapServers}' -n accuknox-dev-kafka","title":"Get bootstrap server endpoint"},{"location":"accuknox-onprem/kafka-deploy/#get_ca","text":"kubectl get secret dev-kafka-cluster-ca-cert -o jsonpath = '{.data.ca\\.p12}' -n accuknox-dev-kafka | base64 -d > ca.p12","title":"Get CA"},{"location":"accuknox-onprem/kafka-deploy/#get_ca_password","text":"kubectl get secret dev-kafka-cluster-ca-cert -o jsonpath = '{.data.ca\\.password}' -n accuknox-dev-kafka | base64 -d > ca.password","title":"Get CA Password"},{"location":"accuknox-onprem/kafka-deploy/#get_user_cert","text":"kubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath = '{.data.user\\.p12}' | base64 -d > user.p12","title":"Get User Cert"},{"location":"accuknox-onprem/kafka-deploy/#get_user_password","text":"kubectl get secret/node-event-feeder-common -n accuknox-dev-kafka -o jsonpath = '{.data.user\\.password}' | base64 -d > user.password","title":"Get user password"},{"location":"accuknox-onprem/kafka-deploy/#convert_userp12_into_base64","text":"cat user.p12 | base64 > user.p12.base64","title":"Convert user.p12 into base64"},{"location":"accuknox-onprem/kafka-deploy/#convert_cap12_into_base64","text":"cat ca.p12 | base64 > ca.p12.base64","title":"Convert ca.p12 into base64"},{"location":"accuknox-onprem/kafka-deploy/#convert_capassword_into_base64","text":"cat ca.password | base64 > ca.password.base64","title":"Convert ca.password into base64"},{"location":"accuknox-onprem/kafka-deploy/#convert_userpassword_into_base64","text":"cat user.password | base64 > user.password.base64","title":"Convert user.password into base64"},{"location":"accuknox-onprem/kafka-deploy/#convert_p12_to_pem","text":"openssl pkcs12 -in ca.p12 -out ca.pem","title":"Convert p12 to pem"},{"location":"accuknox-onprem/kafka-deploy/#convert_capem_to_base64","text":"cat ca.pem | base64 > ca.pem.base64 Note: ca.p12, ca.password, user.p12 and user.password are required to be used in Java based applications. For Go based applications, use ca.pem, user.p12 and user.password. For use in Kubernetes, use the base64 versions of respective files. FQDN (K8\u2019s Service name) Value for Internal Cluster application connectivity. FQDN : dev-kafka-kafka-bootstrap.accuknox-dev-kafka.svc.cluster.local:9092","title":"Convert ca.pem to base64"},{"location":"accuknox-onprem/kafka/","text":"Strimzi simplifies the process of running Apache Kafka in a Kubernetes cluster. Features \u00b6 The underlying data stream-processing capabilities and component architecture of Kafka can deliver: Microservices and other applications to share data with extremely high throughput and low latency Message ordering guarantees Message rewind/replay from data storage to reconstruct an application state Message compaction to remove old records when using a key-value log Horizontal scalability in a cluster configuration Replication of data to control fault tolerance Retention of high volumes of data for immediate access For more reference: click here..","title":"What is Kafka?"},{"location":"accuknox-onprem/kafka/#features","text":"The underlying data stream-processing capabilities and component architecture of Kafka can deliver: Microservices and other applications to share data with extremely high throughput and low latency Message ordering guarantees Message rewind/replay from data storage to reconstruct an application state Message compaction to remove old records when using a key-value log Horizontal scalability in a cluster configuration Replication of data to control fault tolerance Retention of high volumes of data for immediate access For more reference: click here..","title":"Features"},{"location":"accuknox-onprem/mysql-deploy/","text":"Note: \u00b6 Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - mysql:true Labels - mysql:true Add accuknox repository to install Mysql Percona helm package: helm repo add accuknox-prerequisites https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-prerequisites helm repo update helm search repo accuknox-prerequisites helm pull accuknox-prerequisites/mysql\u2014untar cd mysql-percona-chart kubectl create namespace accuknox-dev-mysql kubectl config set-context $( kubectl config current-context ) --namespace = accuknox-dev-mysql cd accuknox-dev-mysql kubectl apply -f bundle.yaml kubectl apply -f cr.yaml kubectl apply -f secrets.yaml kubectl apply -f ssl-secrets.yaml kubectl apply -f backup-s3.yaml After following steps the above steps, you will see a similar image as above Run a sanitary test with below commands at the mysql namespace kubectl run -i --rm --tty percona-client --image = percona:8.0 --restart = Never -- bash -il mysql -h accuknox-dev-mysql-haproxy -uroot -proot_password Update the passwords in secret.yaml file and run below command kubectl apply -f secrets.yaml Optional To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files. FQDN: For K8\u2019s Service name accuknox-dev-mysql-haproxy.accuknox-dev-mysql.svc.cluster.local \u00b6","title":"How to deploy Percona-MySQL?"},{"location":"accuknox-onprem/mysql-deploy/#note","text":"Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - mysql:true Labels - mysql:true Add accuknox repository to install Mysql Percona helm package: helm repo add accuknox-prerequisites https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-prerequisites helm repo update helm search repo accuknox-prerequisites helm pull accuknox-prerequisites/mysql\u2014untar cd mysql-percona-chart kubectl create namespace accuknox-dev-mysql kubectl config set-context $( kubectl config current-context ) --namespace = accuknox-dev-mysql cd accuknox-dev-mysql kubectl apply -f bundle.yaml kubectl apply -f cr.yaml kubectl apply -f secrets.yaml kubectl apply -f ssl-secrets.yaml kubectl apply -f backup-s3.yaml After following steps the above steps, you will see a similar image as above Run a sanitary test with below commands at the mysql namespace kubectl run -i --rm --tty percona-client --image = percona:8.0 --restart = Never -- bash -il mysql -h accuknox-dev-mysql-haproxy -uroot -proot_password Update the passwords in secret.yaml file and run below command kubectl apply -f secrets.yaml Optional To configure backup with gcs add the HMAC keys in backup-s3.yaml, change the bucket name in cr.yaml and cron can be changed as required cr.yaml files. FQDN: For K8\u2019s Service name","title":"Note:"},{"location":"accuknox-onprem/mysql-deploy/#accuknox-dev-mysql-haproxyaccuknox-dev-mysqlsvcclusterlocal","text":"","title":"accuknox-dev-mysql-haproxy.accuknox-dev-mysql.svc.cluster.local"},{"location":"accuknox-onprem/mysql/","text":"Percona Distribution for Mysql Operator is an open-source drop in replacement for MySQL Enterprise with synchronous replication running on Kubernetes. It automates the deployment and management of the members in your Percona XtraDB Cluster environment. It can be used to instantiate a new Percona XtraDB Cluster, or to scale an existing environment. Consult the documentation on the Percona Distribution for Mysql Operator for complete details on capabilities and options. Supported Features \u00b6 Scale Your Cluster change the size parameter to add or remove members of the cluster. Three is the minimum recommended size for a functioning cluster. Automate Your Backups configure cluster backups to run on a scheduled basis. Backups can be stored on a persistent volume or S3-compatible storage. Leverage Point-in-time recovery to reduce RPO/RTO. Proxy integration choose HAProxy or ProxySQL as a proxy in front of the Percona XtraDB Cluster. Proxies are deployed and configured automatically with the Operator. Common Configurations \u00b6 Set Resource Limits - set limitation on requests to CPU and memory resources. Customize Storage - set the desired Storage Class and Access Mode for your database cluster data. Control Scheduling - define how your PXC Pods are scheduled onto the K8S cluster with tolerations, pod disruption budgets, node selector and affinity settings. Automatic synchronization of MySQL users with ProxySQL Fully automated minor version updates (Smart Update) Update Reader members before Writer member at cluster upgrades For more reference: click here..","title":"What is Percona-MySQL?"},{"location":"accuknox-onprem/mysql/#supported_features","text":"Scale Your Cluster change the size parameter to add or remove members of the cluster. Three is the minimum recommended size for a functioning cluster. Automate Your Backups configure cluster backups to run on a scheduled basis. Backups can be stored on a persistent volume or S3-compatible storage. Leverage Point-in-time recovery to reduce RPO/RTO. Proxy integration choose HAProxy or ProxySQL as a proxy in front of the Percona XtraDB Cluster. Proxies are deployed and configured automatically with the Operator.","title":"Supported Features"},{"location":"accuknox-onprem/mysql/#common_configurations","text":"Set Resource Limits - set limitation on requests to CPU and memory resources. Customize Storage - set the desired Storage Class and Access Mode for your database cluster data. Control Scheduling - define how your PXC Pods are scheduled onto the K8S cluster with tolerations, pod disruption budgets, node selector and affinity settings. Automatic synchronization of MySQL users with ProxySQL Fully automated minor version updates (Smart Update) Update Reader members before Writer member at cluster upgrades For more reference: click here..","title":"Common Configurations"},{"location":"accuknox-onprem/pinot-deploy/","text":"Note: \u00b6 Pinot Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - pinot:true Labels - pinot:true Add accuknox repository to install Pinot helm package helm repo add accuknox-prerequisites https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-prerequisites helm repo update helm search repo accuknox-prerequisites helm pull accuknox-prerequisites/pinot --untar kubectl create namespace accuknox-dev-pinot kubectl config set-context \u2013current --namespace = accuknox-dev-pinot helm install accuknox-dev-pinot accuknox-pinot-dev/ kubectl get all","title":"How to deploy Pinot?"},{"location":"accuknox-onprem/pinot-deploy/#note","text":"Pinot Operator Deployment 1. Don't change the namespace name because if you change the namespace - You need to change the service name , If the service name is changed ,then you need to change the microservice configmap files . eg: app.yaml. Please create a node pool on EKS / GKE / AKS (or) on-premises worker nodes with below Taints and labels Taints - pinot:true Labels - pinot:true Add accuknox repository to install Pinot helm package helm repo add accuknox-prerequisites https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-prerequisites helm repo update helm search repo accuknox-prerequisites helm pull accuknox-prerequisites/pinot --untar kubectl create namespace accuknox-dev-pinot kubectl config set-context \u2013current --namespace = accuknox-dev-pinot helm install accuknox-dev-pinot accuknox-pinot-dev/ kubectl get all","title":"Note:"},{"location":"accuknox-onprem/pinot/","text":"Pinot is a real-time distributed OLAP datastore, purpose-built to provide ultra low-latency analytics, even at extremely high throughput. It can ingest directly from streaming data sources - such as Apache Kafka and Amazon Kinesis - and make the events available for querying instantly. It can also ingest from batch data sources such as Hadoop HDFS, Amazon S3, Azure ADLS, and Google Cloud Storage. At the heart of the system is a columnar store, with several smart indexing and pre-aggregation techniques for low latency. This makes Pinot the most perfect fit for user-facing realtime analytics. At the same time, Pinot is also a great choice for other analytical use-cases, such as internal dashboards, anomaly detection, and ad-hoc data exploration. For more reference: click here..","title":"What is Pinot?"},{"location":"accuknox-onprem/ui-deploy/","text":"Add accuknox repository to install UI on VM \u00b6 helm repo add accuknox-ui https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-ui helm repo update helm search repo accuknox-ui helm pull accuknox-ui/ui-build --untar Step 1 : Install nginx application on VM Step 2: Configure the certmanager(https)(eg: letsencrypt) Step 3: tar -xvf build.tar.gz Step 4: sudo cp -rvf build/* /usr/share/nginx/html/ Step 5: Start the service","title":"UI deployment"},{"location":"accuknox-onprem/ui-deploy/#add_accuknox_repository_to_install_ui_on_vm","text":"helm repo add accuknox-ui https://accuknox-user:5U:u8~wu-b@agents.accuknox.com/repository/accuknox-ui helm repo update helm search repo accuknox-ui helm pull accuknox-ui/ui-build --untar Step 1 : Install nginx application on VM Step 2: Configure the certmanager(https)(eg: letsencrypt) Step 3: tar -xvf build.tar.gz Step 4: sudo cp -rvf build/* /usr/share/nginx/html/ Step 5: Start the service","title":"Add accuknox repository to install UI on VM"},{"location":"extensions/code-hilite/","text":"CodeHilite \u00b6 CodeHilite - Material for MkDocs Supported languages - Pygments Configure mkdocs.yml \u00b6 markdown_extensions: - codehilite","title":"CodeHilite"},{"location":"extensions/code-hilite/#codehilite","text":"CodeHilite - Material for MkDocs Supported languages - Pygments","title":"CodeHilite"},{"location":"extensions/code-hilite/#configure_mkdocsyml","text":"markdown_extensions: - codehilite","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/","text":"Footnote \u00b6 Footnotes - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - footnotes Example \u00b6 Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Footnote"},{"location":"extensions/footnote/#footnote","text":"Footnotes - Material for MkDocs","title":"Footnote"},{"location":"extensions/footnote/#configure_mkdocsyml","text":"markdown_extensions: - footnotes","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/#example","text":"Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Example"},{"location":"extensions/mathjax/","text":"MathJax \u00b6 PyMdown - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - mdx_math: enable_dollar_delimiter: True Example code \u00b6 $$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$ Example rendering \u00b6 P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha","title":"MathJax"},{"location":"extensions/mathjax/#mathjax","text":"PyMdown - Material for MkDocs","title":"MathJax"},{"location":"extensions/mathjax/#configure_mkdocsyml","text":"markdown_extensions: - mdx_math: enable_dollar_delimiter: True","title":"Configure mkdocs.yml"},{"location":"extensions/mathjax/#example_code","text":"$$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$","title":"Example code"},{"location":"extensions/mathjax/#example_rendering","text":"P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha","title":"Example rendering"},{"location":"getting-started/Logs/","text":"Logs and Triggers \u00b6 1. What Kind of Logs: \u00b6 The logs generated from each component like Network / System / Data Protection / Anamoly Detection can be viewed under Logs section. The logs can be viewed for independent workloads like K8's or VM's Each of the component should be up and Running in the installed cluster for the logs to be seen in the Workspace . NOTE: It's assumed that All Agents is running on cluster if not kindly go through this section 2. Triggers: \u00b6 An Alert trigger is a rule which triggers information transfer through a known channel. Customer can send information [log] in Real Time when it occurs or on specified Frequency. Frequency is configurable which can be Once In a Day/ Week / Month. Predefined Filters (or) pre-saved global filters are search queries on logs that is already available in the system. Example : show logs for traffic direction = \"EGRESS\" 3. Configuration of Alert Triggers: \u00b6 On the Logs page, after choosing specific log filter click on 'Create Trigger' button. The below fields needs to be entered with appropriate data: Name: Enter the name for the trigger. You can set any name without special characters. When to Initiate: The frequency of the trigger as Real Time / . Status: Enter the severity for the trigger. Search Filter Data : The filter log chosen in automatically populated here. This is optional. Predefined queries: The list of predefined queries for this workspace is shown as default. Notification Channel: Select the integration channel that needs to receive logs. Can be Slack/ Splunk / Cloudwatch / Elastic. (Note: Channel Integrations needs to be done beforehand) Save: Click on Save for the trigger to get stored in database. a. List Of Triggers: \u00b6 Select Triggers from the left navigation to view the list of triggers created. (If Default params needs to be modified) The below fields for each trigger can be viewed. Alert Trigger : Name of the Trigger Created At : Time of Trigger Creation Channel : Integration Channel Name Enable/Disable: Enabling / Disabling the trigger to forward the logs into Integration Channel. Details : The filter query of the associated Trigger. b. Actions: \u00b6 Select the below Actions on the triggers to check working of logs forwarding. (If Default params needs to be modified) The below actions for each trigger can be selected. Enable : Enabling the trigger to forward the logs into Integration Channel. Disable : Disabling the trigger to not forward the logs into Integration Channel. Delete : Delete the Logs Trigger Export : Exporting the triggers in PDF format. 4. Logs Forwarding: \u00b6 For each Enabled Trigger, please check the integrated channel to view the logs. The Rule Engine matches the real time logs against the triggers created. Frequency >> Real Time The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel as it happens. Frequency >> Once in A Day The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Day. Frequency >> Once in A Month The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Month. Frequency >> Once in A Week The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Week. NOTE: Frequency reduces channel noise / and safely provides only one informational log on the channels based on frequency time period.","title":"Logs and Triggers"},{"location":"getting-started/Logs/#logs_and_triggers","text":"","title":"Logs and Triggers"},{"location":"getting-started/Logs/#1_what_kind_of_logs","text":"The logs generated from each component like Network / System / Data Protection / Anamoly Detection can be viewed under Logs section. The logs can be viewed for independent workloads like K8's or VM's Each of the component should be up and Running in the installed cluster for the logs to be seen in the Workspace . NOTE: It's assumed that All Agents is running on cluster if not kindly go through this section","title":"1. What Kind of Logs:"},{"location":"getting-started/Logs/#2_triggers","text":"An Alert trigger is a rule which triggers information transfer through a known channel. Customer can send information [log] in Real Time when it occurs or on specified Frequency. Frequency is configurable which can be Once In a Day/ Week / Month. Predefined Filters (or) pre-saved global filters are search queries on logs that is already available in the system. Example : show logs for traffic direction = \"EGRESS\"","title":"2. Triggers:"},{"location":"getting-started/Logs/#3_configuration_of_alert_triggers","text":"On the Logs page, after choosing specific log filter click on 'Create Trigger' button. The below fields needs to be entered with appropriate data: Name: Enter the name for the trigger. You can set any name without special characters. When to Initiate: The frequency of the trigger as Real Time / . Status: Enter the severity for the trigger. Search Filter Data : The filter log chosen in automatically populated here. This is optional. Predefined queries: The list of predefined queries for this workspace is shown as default. Notification Channel: Select the integration channel that needs to receive logs. Can be Slack/ Splunk / Cloudwatch / Elastic. (Note: Channel Integrations needs to be done beforehand) Save: Click on Save for the trigger to get stored in database.","title":"3. Configuration of Alert Triggers:"},{"location":"getting-started/Logs/#a_list_of_triggers","text":"Select Triggers from the left navigation to view the list of triggers created. (If Default params needs to be modified) The below fields for each trigger can be viewed. Alert Trigger : Name of the Trigger Created At : Time of Trigger Creation Channel : Integration Channel Name Enable/Disable: Enabling / Disabling the trigger to forward the logs into Integration Channel. Details : The filter query of the associated Trigger.","title":"a. List Of Triggers:"},{"location":"getting-started/Logs/#b_actions","text":"Select the below Actions on the triggers to check working of logs forwarding. (If Default params needs to be modified) The below actions for each trigger can be selected. Enable : Enabling the trigger to forward the logs into Integration Channel. Disable : Disabling the trigger to not forward the logs into Integration Channel. Delete : Delete the Logs Trigger Export : Exporting the triggers in PDF format.","title":"b. Actions:"},{"location":"getting-started/Logs/#4_logs_forwarding","text":"For each Enabled Trigger, please check the integrated channel to view the logs. The Rule Engine matches the real time logs against the triggers created. Frequency >> Real Time The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel as it happens. Frequency >> Once in A Day The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Day. Frequency >> Once in A Month The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Month. Frequency >> Once in A Week The Network (OR) System (OR) S3 (OR) Data protection (OR) Anamoly Detection Logs in Real Time can be seen in integrated Channel only Once in a Week. NOTE: Frequency reduces channel noise / and safely provides only one informational log on the channels based on frequency time period.","title":"4. Logs Forwarding:"},{"location":"getting-started/agent-metrics/","text":"Agent Metrics: FileBeat | Kibana | On-prem Metrics \u00b6 1. Status of Feeder Agent running on Cluster: \u00b6 Please run the below command to check if agent and dependent pods are up and running. kubectl get all \u2013n feeder-service All the pods/services should be in Running state. NOTE: It's assumed that Feeder Agent is running on cluster if not kindly go through this section 2. Beats Setup: \u00b6 The agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana . a. Elastic Configuration Parameters: \u00b6 The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://elasticsearch-es-http - name : ELASTICSEARCH_PORT value : \"9200\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"xxxxxxxxxxxxx\" b. Command to be Used: \u00b6 kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST = \u201dhttps://elasticsearch-es-http\u201d c. Update Log Path: \u00b6 To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/value.log 2. Kibana Dashboard \u00b6 Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen. In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this One called Visualize and Another called Dashboard In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them. 3. Metrics: \u00b6 Once the feeder agent starts running, check the logs using below command Kubectl logs \u2013f podname \u2013n feeder-service The logs will push the metric data to GRPC Client / Kafka, and the GRPC server in SaaS platform will be listening to this metrics and can be viewed in Prometheus. (prometheus-dev.accuknox.com) 4. On Prem Metrics: \u00b6 To fetch the metrics in standalone environment, please write a scrape job in Prometheus with feeder service agent as a job name, and scrape the metrics from port (:xxxx) - job_name : \"feeder-pod-agent\" sample_limit : 10000","title":"Agent Metrics"},{"location":"getting-started/agent-metrics/#agent_metrics_filebeat_kibana_on-prem_metrics","text":"","title":"Agent Metrics: FileBeat | Kibana | On-prem Metrics"},{"location":"getting-started/agent-metrics/#1_status_of_feeder_agent_running_on_cluster","text":"Please run the below command to check if agent and dependent pods are up and running. kubectl get all \u2013n feeder-service All the pods/services should be in Running state. NOTE: It's assumed that Feeder Agent is running on cluster if not kindly go through this section","title":"1. Status of Feeder Agent running on Cluster:"},{"location":"getting-started/agent-metrics/#2_beats_setup","text":"The agent will be spinned along with Filebeat running along as a sidecar. The filebeat configuration file in the package can be updated to specific Elastic instances, and logs can be viewed in Kibana .","title":"2. Beats Setup:"},{"location":"getting-started/agent-metrics/#a_elastic_configuration_parameters","text":"The below Configuration parameters can be updated for elastic configuration. (If Default params needs to be modified) - name : ELASTICSEARCH_HOST value : https://elasticsearch-es-http - name : ELASTICSEARCH_PORT value : \"9200\" - name : ELASTICSEARCH_USERNAME value : \"elastic\" - name : ELASTICSEARCH_PASSWORD value : \"xxxxxxxxxxxxx\"","title":"a. Elastic Configuration Parameters:"},{"location":"getting-started/agent-metrics/#b_command_to_be_used","text":"kubectl set env deploy/feeder -n feeder-service ELASTICSEARCH_HOST = \u201dhttps://elasticsearch-es-http\u201d","title":"b. Command to be Used:"},{"location":"getting-started/agent-metrics/#c_update_log_path","text":"To Update the Log path configured, please modify the below log input path under file beat inputs. filebeat.inputs : - type : container paths : - /log_output/value.log","title":"c. Update Log Path:"},{"location":"getting-started/agent-metrics/#2_kibana_dashboard","text":"Once the filebeat starts listening, an index will be created or updated on the elastic configured and the pushed logs can be seen. In order to create a dashboard, you will need to build visualizations. Kibana has two panels for this One called Visualize and Another called Dashboard In order to create your dashboard, you will first create every individual visualization with the Visualize panel and save them.","title":"2. Kibana Dashboard"},{"location":"getting-started/agent-metrics/#3_metrics","text":"Once the feeder agent starts running, check the logs using below command Kubectl logs \u2013f podname \u2013n feeder-service The logs will push the metric data to GRPC Client / Kafka, and the GRPC server in SaaS platform will be listening to this metrics and can be viewed in Prometheus. (prometheus-dev.accuknox.com)","title":"3. Metrics:"},{"location":"getting-started/agent-metrics/#4_on_prem_metrics","text":"To fetch the metrics in standalone environment, please write a scrape job in Prometheus with feeder service agent as a job name, and scrape the metrics from port (:xxxx) - job_name : \"feeder-pod-agent\" sample_limit : 10000","title":"4. On Prem Metrics:"},{"location":"getting-started/channel-integration/","text":"Channel Integration \u00b6 Channel Integrations is the fourth sub-section of Workspace Manager. This section is used to integrate external services with AccuKnox so you can export logs as well as metrics. These services include: Webhooks Slack Jira Elastic Search Pagerduty Syslog Splunk Email ServiceNow AWS CloudWatch Choose any one of the services and click the Integrate Now button. 1. Integration of Slack: \u00b6 a. Prerequisites: \u00b6 You need a valid and active account in Slack. After logging into your Slack channel, you must generate a Hook URL. [Note]: If you don\u2019t know how to get Hook URL then click this link and follow the steps. b. Steps to Integrate: \u00b6 Goto Channel Integration URL Click the Integrate Now button inside Slack Here you'll be able to see these entries: Integration Name: Enter the name for the integration. You can set any name. Hook URL: Enter your slack hook URL here. Sender Name: Enter the sender name here. Channel Name: Enter your slack channel name here. Message Title: You can set a message title using this input field. This is optional. Tags to be sent with alerts: You can set tags using this input field. This is optional. Once you fill every field then click the button this will test whether your integration is working or not. Click the Save button. 2. Integration of Splunk: \u00b6 a. Prerequisites \u00b6 You need a Splunk HTTP event collector URL for this Integration. [Note]: If you don\u2019t know how to get Splunk HTTP event collector URL then click this link b. Steps to Integrate: \u00b6 Goto Channel Integration URL Click the Integrate Now button inside Splunk Here you'll be able to see these entries: Integration Name: Enter the name for the integration. You can set any name. Splunk HTTP event collector URL: Enter your Splunk HTTP event collector URL here. Token: Enter your Splunk Token here. Source: Enter your Splunk channel name here. Index: Enter your Splunk Index here. Source Type: Enter your Source Type here. Enable HTTPS: If you want HTTPS service then enable this button. Enable TLS Verify: If you want TLS service then enable this button. Once you fill every field then click the button this will test whether your integration is working or not. Click the Save button.","title":"What is Channel Integration"},{"location":"getting-started/channel-integration/#channel_integration","text":"Channel Integrations is the fourth sub-section of Workspace Manager. This section is used to integrate external services with AccuKnox so you can export logs as well as metrics. These services include: Webhooks Slack Jira Elastic Search Pagerduty Syslog Splunk Email ServiceNow AWS CloudWatch Choose any one of the services and click the Integrate Now button.","title":"Channel Integration"},{"location":"getting-started/channel-integration/#1_integration_of_slack","text":"","title":"1. Integration of Slack:"},{"location":"getting-started/channel-integration/#a_prerequisites","text":"You need a valid and active account in Slack. After logging into your Slack channel, you must generate a Hook URL. [Note]: If you don\u2019t know how to get Hook URL then click this link and follow the steps.","title":"a. Prerequisites:"},{"location":"getting-started/channel-integration/#b_steps_to_integrate","text":"Goto Channel Integration URL Click the Integrate Now button inside Slack Here you'll be able to see these entries: Integration Name: Enter the name for the integration. You can set any name. Hook URL: Enter your slack hook URL here. Sender Name: Enter the sender name here. Channel Name: Enter your slack channel name here. Message Title: You can set a message title using this input field. This is optional. Tags to be sent with alerts: You can set tags using this input field. This is optional. Once you fill every field then click the button this will test whether your integration is working or not. Click the Save button.","title":"b. Steps to Integrate:"},{"location":"getting-started/channel-integration/#2_integration_of_splunk","text":"","title":"2. Integration of Splunk:"},{"location":"getting-started/channel-integration/#a_prerequisites_1","text":"You need a Splunk HTTP event collector URL for this Integration. [Note]: If you don\u2019t know how to get Splunk HTTP event collector URL then click this link","title":"a. Prerequisites"},{"location":"getting-started/channel-integration/#b_steps_to_integrate_1","text":"Goto Channel Integration URL Click the Integrate Now button inside Splunk Here you'll be able to see these entries: Integration Name: Enter the name for the integration. You can set any name. Splunk HTTP event collector URL: Enter your Splunk HTTP event collector URL here. Token: Enter your Splunk Token here. Source: Enter your Splunk channel name here. Index: Enter your Splunk Index here. Source Type: Enter your Source Type here. Enable HTTPS: If you want HTTPS service then enable this button. Enable TLS Verify: If you want TLS service then enable this button. Once you fill every field then click the button this will test whether your integration is working or not. Click the Save button.","title":"b. Steps to Integrate:"},{"location":"getting-started/cilium-install/","text":"Cilium: Deployment Guide \u00b6 Deployment Steps for Cilium & Hubble CLI \u00b6 1. Download and install Cilium CLI \u00b6 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } 2. Install Cilium \u00b6 cilium install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings. 3. Validate the Installation \u00b6 a. [Optional] To validate that Cilium has been properly installed, you can run: \u00b6 cilium status --wait b. [Optional] Run the following command to validate that your cluster has proper network connectivity: \u00b6 cilium connectivity test Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89 4. Setting up Hubble Observability \u00b6 a. Enable Hubble in Cilium \u00b6 cilium hubble enable b. Install the Hubble CLI Client \u00b6 export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } 5. Getting Alerts/Telemetry from Cilium \u00b6 a. Enable port-forwarding for Cilium Hubble relay \u00b6 cilium hubble port-forward & b. Observing logs using hubble cli \u00b6 hubble observe","title":"Cilium: Deployment Guide"},{"location":"getting-started/cilium-install/#cilium_deployment_guide","text":"","title":"Cilium: Deployment Guide"},{"location":"getting-started/cilium-install/#deployment_steps_for_cilium_hubble_cli","text":"","title":"Deployment Steps for Cilium &amp; Hubble CLI"},{"location":"getting-started/cilium-install/#1_download_and_install_cilium_cli","text":"curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum }","title":"1. Download and install Cilium CLI"},{"location":"getting-started/cilium-install/#2_install_cilium","text":"cilium install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings.","title":"2. Install Cilium"},{"location":"getting-started/cilium-install/#3_validate_the_installation","text":"","title":"3. Validate the Installation"},{"location":"getting-started/cilium-install/#a_optional_to_validate_that_cilium_has_been_properly_installed_you_can_run","text":"cilium status --wait","title":"a. [Optional] To validate that Cilium has been properly installed, you can run:"},{"location":"getting-started/cilium-install/#b_optional_run_the_following_command_to_validate_that_your_cluster_has_proper_network_connectivity","text":"cilium connectivity test Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89","title":"b. [Optional] Run the following command to validate that your cluster has proper network connectivity:"},{"location":"getting-started/cilium-install/#4_setting_up_hubble_observability","text":"","title":"4. Setting up Hubble Observability"},{"location":"getting-started/cilium-install/#a_enable_hubble_in_cilium","text":"cilium hubble enable","title":"a. Enable Hubble in Cilium"},{"location":"getting-started/cilium-install/#b_install_the_hubble_cli_client","text":"export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"b. Install the Hubble CLI Client"},{"location":"getting-started/cilium-install/#5_getting_alertstelemetry_from_cilium","text":"","title":"5. Getting Alerts/Telemetry from Cilium"},{"location":"getting-started/cilium-install/#a_enable_port-forwarding_for_cilium_hubble_relay","text":"cilium hubble port-forward &","title":"a. Enable port-forwarding for Cilium Hubble relay"},{"location":"getting-started/cilium-install/#b_observing_logs_using_hubble_cli","text":"hubble observe","title":"b. Observing logs using hubble cli"},{"location":"getting-started/docker/","text":"Start with Docker \u00b6 Public docker image \u00b6 Package peaceiris/mkdocs-material docker-compose \u00b6 Here is an example docker-compose.yml Please check the latest tag before you go. docker-compose up Go to http://localhost:8000/","title":"Start with Docker"},{"location":"getting-started/docker/#start_with_docker","text":"","title":"Start with Docker"},{"location":"getting-started/docker/#public_docker_image","text":"Package peaceiris/mkdocs-material","title":"Public docker image"},{"location":"getting-started/docker/#docker-compose","text":"Here is an example docker-compose.yml Please check the latest tag before you go. docker-compose up Go to http://localhost:8000/","title":"docker-compose"},{"location":"getting-started/workspace-manager/","text":"Workspace Manager in AccuKnox \u00b6 In Workspace Manager, user can perform various tasks such as managing users and roles, onboard cluster and adding third-party integrations etc. The workspace manager has 4 subsections. These four sections are: User Management Role-Based Access Control Onboard Cluster Channel Integrations","title":"What is Workspace Manager"},{"location":"getting-started/workspace-manager/#workspace_manager_in_accuknox","text":"In Workspace Manager, user can perform various tasks such as managing users and roles, onboard cluster and adding third-party integrations etc. The workspace manager has 4 subsections. These four sections are: User Management Role-Based Access Control Onboard Cluster Channel Integrations","title":"Workspace Manager in AccuKnox"},{"location":"open-source/cilium-install/","text":"Cilium: Deployment Guide \u00b6 Deployment Steps for Cilium & Hubble CLI \u00b6 1. Download and install Cilium CLI \u00b6 curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum } 2. Install Cilium \u00b6 cilium install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings. 3. Validate the Installation \u00b6 a. [Optional] To validate that Cilium has been properly installed, you can run: \u00b6 cilium status --wait b. [Optional] Run the following command to validate that your cluster has proper network connectivity: \u00b6 cilium connectivity test Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89 4. Setting up Hubble Observability \u00b6 a. Enable Hubble in Cilium \u00b6 cilium hubble enable b. Install the Hubble CLI Client \u00b6 export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum } 5. Getting Alerts/Telemetry from Cilium \u00b6 a. Enable port-forwarding for Cilium Hubble relay \u00b6 cilium hubble port-forward & b. Observing logs using hubble cli \u00b6 hubble observe","title":"Installing Cilium"},{"location":"open-source/cilium-install/#cilium_deployment_guide","text":"","title":"Cilium: Deployment Guide"},{"location":"open-source/cilium-install/#deployment_steps_for_cilium_hubble_cli","text":"","title":"Deployment Steps for Cilium &amp; Hubble CLI"},{"location":"open-source/cilium-install/#1_download_and_install_cilium_cli","text":"curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz { ,.sha256sum }","title":"1. Download and install Cilium CLI"},{"location":"open-source/cilium-install/#2_install_cilium","text":"cilium install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings.","title":"2. Install Cilium"},{"location":"open-source/cilium-install/#3_validate_the_installation","text":"","title":"3. Validate the Installation"},{"location":"open-source/cilium-install/#a_optional_to_validate_that_cilium_has_been_properly_installed_you_can_run","text":"cilium status --wait","title":"a. [Optional] To validate that Cilium has been properly installed, you can run:"},{"location":"open-source/cilium-install/#b_optional_run_the_following_command_to_validate_that_your_cluster_has_proper_network_connectivity","text":"cilium connectivity test Congratulations! You have a fully functional Kubernetes cluster with Cilium. \ud83c\udf89","title":"b. [Optional] Run the following command to validate that your cluster has proper network connectivity:"},{"location":"open-source/cilium-install/#4_setting_up_hubble_observability","text":"","title":"4. Setting up Hubble Observability"},{"location":"open-source/cilium-install/#a_enable_hubble_in_cilium","text":"cilium hubble enable","title":"a. Enable Hubble in Cilium"},{"location":"open-source/cilium-install/#b_install_the_hubble_cli_client","text":"export HUBBLE_VERSION = $( curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt ) curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/ $HUBBLE_VERSION /hubble-linux-amd64.tar.gz { ,.sha256sum } sha256sum --check hubble-linux-amd64.tar.gz.sha256sum sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin rm hubble-linux-amd64.tar.gz { ,.sha256sum }","title":"b. Install the Hubble CLI Client"},{"location":"open-source/cilium-install/#5_getting_alertstelemetry_from_cilium","text":"","title":"5. Getting Alerts/Telemetry from Cilium"},{"location":"open-source/cilium-install/#a_enable_port-forwarding_for_cilium_hubble_relay","text":"cilium hubble port-forward &","title":"a. Enable port-forwarding for Cilium Hubble relay"},{"location":"open-source/cilium-install/#b_observing_logs_using_hubble_cli","text":"hubble observe","title":"b. Observing logs using hubble cli"},{"location":"open-source/kubearmor-install/","text":"KubeArmor: Deployment Guide \u00b6 Deployment Steps for KubeArmor & kArmor CLI \u00b6 1. Download and install karmor CLI \u00b6 curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin 2. Install KubeArmor \u00b6 karmor install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings. 3. Deploying sample app and policies \u00b6 a. Deploy sample multiubuntu app \u00b6 kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/multiubuntu-deployment.yaml b. Deploy sample policies \u00b6 kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/security-policies/ksp-group-1-proc-path-block.yaml This sample policy blocks execution of sleep command in ubuntu-1 pods. c. Simulate policy violation \u00b6 $ kubectl -n multiubuntu exec -it POD_NAME_FOR_UBUNTU_1 -- bash # sleep 1 ( Permission Denied ) Substitute POD_NAME_FOR_UBUNTU_1 with the actual pod name from kubectl get pods -n multiubuntu . 4. Getting Alerts/Telemetry from KubeArmor \u00b6 a. Enable port-forwarding for KubeArmor relay \u00b6 kubectl port-forward -n kube-system svc/kubearmor 32767 :32767 b. Observing logs using karmor cli \u00b6 karmor log K8s platforms tested \u00b6 Google Kubernetes Engine (GKE) Container Optimized OS (COS) GKE Ubuntu image Amazon Elastic Kubernetes Service (EKS) Self-managed (on-prem) k8s Local k8s engines (microk8s, k3s, minikube)","title":"Installing KubeArmor"},{"location":"open-source/kubearmor-install/#kubearmor_deployment_guide","text":"","title":"KubeArmor: Deployment Guide"},{"location":"open-source/kubearmor-install/#deployment_steps_for_kubearmor_karmor_cli","text":"","title":"Deployment Steps for KubeArmor &amp; kArmor CLI"},{"location":"open-source/kubearmor-install/#1_download_and_install_karmor_cli","text":"curl -sfL https://raw.githubusercontent.com/kubearmor/kubearmor-client/main/install.sh | sudo sh -s -- -b /usr/local/bin","title":"1. Download and install karmor CLI"},{"location":"open-source/kubearmor-install/#2_install_kubearmor","text":"karmor install It is assumed that the k8s cluster is already present/reachable and the user has rights to create service-accounts and cluster-role-bindings.","title":"2. Install KubeArmor"},{"location":"open-source/kubearmor-install/#3_deploying_sample_app_and_policies","text":"","title":"3. Deploying sample app and policies"},{"location":"open-source/kubearmor-install/#a_deploy_sample_multiubuntu_app","text":"kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/multiubuntu-deployment.yaml","title":"a. Deploy sample multiubuntu app"},{"location":"open-source/kubearmor-install/#b_deploy_sample_policies","text":"kubectl apply -f https://raw.githubusercontent.com/kubearmor/KubeArmor/master/examples/multiubuntu/security-policies/ksp-group-1-proc-path-block.yaml This sample policy blocks execution of sleep command in ubuntu-1 pods.","title":"b. Deploy sample policies"},{"location":"open-source/kubearmor-install/#c_simulate_policy_violation","text":"$ kubectl -n multiubuntu exec -it POD_NAME_FOR_UBUNTU_1 -- bash # sleep 1 ( Permission Denied ) Substitute POD_NAME_FOR_UBUNTU_1 with the actual pod name from kubectl get pods -n multiubuntu .","title":"c. Simulate policy violation"},{"location":"open-source/kubearmor-install/#4_getting_alertstelemetry_from_kubearmor","text":"","title":"4. Getting Alerts/Telemetry from KubeArmor"},{"location":"open-source/kubearmor-install/#a_enable_port-forwarding_for_kubearmor_relay","text":"kubectl port-forward -n kube-system svc/kubearmor 32767 :32767","title":"a. Enable port-forwarding for KubeArmor relay"},{"location":"open-source/kubearmor-install/#b_observing_logs_using_karmor_cli","text":"karmor log","title":"b. Observing logs using karmor cli"},{"location":"open-source/kubearmor-install/#k8s_platforms_tested","text":"Google Kubernetes Engine (GKE) Container Optimized OS (COS) GKE Ubuntu image Amazon Elastic Kubernetes Service (EKS) Self-managed (on-prem) k8s Local k8s engines (microk8s, k3s, minikube)","title":"K8s platforms tested"},{"location":"open-source/what-is-cilium/","text":"Cilium | eBPF-based Networking, Observability, and Security \u00b6 Cilium is an open source project to provide eBPF-based networking, security, and observability for cloud native environments such as Kubernetes clusters and other container orchestration platforms. We are focused on adding value to Cilium in the following areas by using: Extensible Identity solution based on SPIFFE standards Improving policy audit handling Improving policy telemetry and statistics collection to fit realistic scenarios Policy discovery tools.","title":"What is Cilium"},{"location":"open-source/what-is-cilium/#cilium_ebpf-based_networking_observability_and_security","text":"Cilium is an open source project to provide eBPF-based networking, security, and observability for cloud native environments such as Kubernetes clusters and other container orchestration platforms. We are focused on adding value to Cilium in the following areas by using: Extensible Identity solution based on SPIFFE standards Improving policy audit handling Improving policy telemetry and statistics collection to fit realistic scenarios Policy discovery tools.","title":"Cilium | eBPF-based Networking, Observability, and Security"},{"location":"open-source/what-is-kubearmor/","text":"KubeArmor | Cloud Native Runtime Security Enforcement System \u00b6 KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level.","title":"What is KubeArmor"},{"location":"open-source/what-is-kubearmor/#kubearmor_cloud_native_runtime_security_enforcement_system","text":"KubeArmor is a cloud-native runtime security enforcement system that restricts the behavior (such as process execution, file access, and networking operation) of containers and nodes at the system level.","title":"KubeArmor | Cloud Native Runtime Security Enforcement System"}]}